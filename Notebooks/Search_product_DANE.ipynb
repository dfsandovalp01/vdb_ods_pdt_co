{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %pip install gradio\n",
    "# # %pip install sentence_transformers\n",
    "# %pip install chromadb\n",
    "# # %pip install zipfile\n",
    "# %pip install pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura VDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dfsandovalp\\AppData\\Local\\miniconda3\\envs\\nlp_base\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\dfsandovalp\\AppData\\Local\\miniconda3\\envs\\nlp_base\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\dfsandovalp\\AppData\\Local\\miniconda3\\envs\\nlp_base\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings_st = SentenceTransformerEmbeddings(\n",
    "    # model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_name=\"hackathon-pln-es/paraphrase-spanish-distilroberta\",\n",
    "    # model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "    # device=\"cuda\",\n",
    "    model_kwargs={\"device\":\"cpu\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dfsandovalp\\AppData\\Local\\miniconda3\\envs\\nlp_base\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "vdb_dir = os.path.join(current_dir, os.pardir, \"data\", \"vdb\", \"chromaPdtGob_pSpDroberta\")\n",
    "# print(os.path.exists(data_dir))\n",
    "vectorstore_chroma = Chroma(\n",
    "    persist_directory=vdb_dir,#NOMBRE_INDICE_CHROMA,\n",
    "    embedding_function=embeddings_st\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "dir_temp_img = os.path.join(os.getcwd(), os.pardir, \"data\", \"temp\", \"img\")\n",
    "dir_temp_pdf = os.path.join(os.getcwd(), os.pardir, \"data\", \"temp\", \"pdf\")\n",
    "\n",
    "def readPageVW(docname, page, dir_temp_img=dir_temp_img):\n",
    "  page_view = None\n",
    "  with pdfplumber.open(docname) as pdf:\n",
    "    page_view = pdf.pages[page-1].to_image()\n",
    "    # dir = os.path.join(os.getcwd(), os.pardir, \"data\", \"temp\", \"img\")\n",
    "    # dir = f\"/content/images\"\n",
    "    # dir = dir.replace('Departamentos/','')#.replace('Departamentos/','')\n",
    "     # Guarda la imagen como .png\n",
    "    os.makedirs(dir_temp_img,exist_ok=True)\n",
    "    page_view.save(f\"{dir_temp_img}/page_{page}_doc_{docname.replace('Departamentos/','').replace('.pdf','.png')}\")\n",
    "  return page_view\n",
    "\n",
    "def readZip(idDoc,page,dir='Departamentos'):\n",
    "# def readZip(idDoc,page,dir=os.path.join(os.getcwd(), \"Departamentos\")):\n",
    "# def readZip(idDoc,page,dir_temp_pdf=dir_temp_pdf):\n",
    "  # for _ in [f for f in os.listdir(dir) if f.endswith('.pdf')]:\n",
    "  #     os.remove(os.path.join(dir, _))\n",
    "  docname = f'{dir}/{idDoc}.pdf'\n",
    "  # docname = os.path.join(dir, f'{idDoc}.pdf')\n",
    "  # print(f'docname: {docname}')\n",
    "  if os.path.exists(docname):\n",
    "      # os.remove(docname)\n",
    "      page_view = readPageVW(docname, page)\n",
    "      # print(f\"{} ha sido eliminado.\")\n",
    "  else:\n",
    "    # print(f\"El archivo {i} no existe.\")\n",
    "    pages = []\n",
    "    bdl_corpus = pd.DataFrame()\n",
    "    page_view = None\n",
    "    # ruta = '/content/drive/MyDrive/Automatizacion ODS 11a1/Pruebas/Departamentos/datos/Departamentos.zip'\n",
    "    ruta = os.path.join(os.getcwd(), os.pardir, \"data\", \"comprimido\", \"Departamentos.zip\")\n",
    "    # Abrir el archivo .zip\n",
    "    with zipfile.ZipFile(ruta, 'r') as zip_ref:\n",
    "      # Listar el contenido del .zip\n",
    "      lista_archivos = zip_ref.namelist()\n",
    "      # print(lista_archivos)\n",
    "\n",
    "      try:\n",
    "        # i = i.replace('Departamentos/','')\n",
    "        zip_ref.extract(docname)\n",
    "        page_view = readPageVW(docname, page)\n",
    "\n",
    "      except Exception as e:\n",
    "        print(f'Error:  {e}')\n",
    "        pass\n",
    "  \n",
    "      \n",
    "  return page_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#departamentos\n",
    "lista_archivos_pdf = ['05.pdf', '08.pdf', '15.pdf', '17.pdf', '18.pdf', '19.pdf', '20.pdf', '23.pdf', '25.pdf', '27.pdf', '41.pdf', '52.pdf', '54.pdf', '63.pdf', '66.pdf',\n",
    " '68.pdf', '73.pdf', '76.pdf', '81.pdf', '85.pdf', '86.pdf', '88.pdf', '91.pdf', '94.pdf', '95.pdf', '97.pdf', '99.pdf']\n",
    "# data_dir = os.path.join(os.getcwd(), os.pardir, \"data\")\n",
    "# print(data_dir)\n",
    "codigos_dane = pd.read_csv(os.path.join(os.getcwd(), os.pardir, \"data\", \"Tabla_codigos_Dane.txt\"), sep='|', dtype=str)\n",
    "codigoDepartamentoPdf = [_.replace('.pdf','') for _ in lista_archivos_pdf]\n",
    "codigos_dane_dpto = codigos_dane.groupby(['CodigoDepartamento', 'NombreDepartamento']).agg({'CodigoMunicipio':'count'}).reset_index()\n",
    "codigos_dane_dpto = codigos_dane_dpto[codigos_dane_dpto.CodigoDepartamento.isin(codigoDepartamentoPdf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dfsandovalp\\AppData\\Local\\miniconda3\\envs\\nlp_base\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('hackathon-pln-es/paraphrase-spanish-distilroberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dfsandovalp\\AppData\\Local\\miniconda3\\envs\\nlp_base\\lib\\site-packages\\gradio\\utils.py:1002: UserWarning: Expected 5 arguments for function <function search at 0x0000022396E5F5E0>, received 4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dfsandovalp\\AppData\\Local\\miniconda3\\envs\\nlp_base\\lib\\site-packages\\gradio\\utils.py:1006: UserWarning: Expected at least 5 arguments for function <function search at 0x0000022396E5F5E0>, received 4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def search(query,filter,dpto,rank, clusters):\n",
    "  # query = \"proyecciones de poblacion\"\n",
    "  # doc1 = None\n",
    "  # if os.path.exists('/content/images'):\n",
    "  if os.path.exists((os.path.join(os.getcwd(), os.pardir, \"data\", \"temp\", \"img\"))):\n",
    "    # for _ in [f for f in os.listdir('/content/images') if f.endswith('.png')]:\n",
    "    for _ in [f for f in os.listdir((os.path.join(os.getcwd(), os.pardir, \"data\", \"temp\", \"img\"))) if f.endswith('.png')]:\n",
    "      os.remove((os.path.join(os.getcwd(), os.pardir, \"data\", \"temp\", \"img\", _)))\n",
    "    # print(f\"{i} ha sido eliminado.\")\n",
    "  if filter == \"Filtro\":\n",
    "    doc1 = codigos_dane_dpto[codigos_dane_dpto.NombreDepartamento == dpto].reset_index(drop=True).loc[0,'CodigoDepartamento']\n",
    "    doc1 = f'{doc1}.pdf'\n",
    "    docs = vectorstore_chroma.similarity_search_with_score(query, k=rank, filter={\"source\": doc1})  # .unique()\n",
    "  else:\n",
    "    docs = vectorstore_chroma.similarity_search_with_score(query, k=rank)\n",
    "\n",
    "\n",
    "\n",
    "  # scores = [round(1 - doc[1]/100,2) for doc in docs ]\n",
    "  scores = [round(doc[1],2) for doc in docs ]\n",
    "  text = [doc[0].page_content for doc in docs ]\n",
    "  page = [doc[0].metadata['page'] for doc in docs ]\n",
    "  source = [doc[0].metadata['source'] for doc in docs ]\n",
    "  idDN = [doc[0].metadata['source'].replace('.pdf','') for doc in docs ]\n",
    "  ids = list(range(0, len(text))) \n",
    "  # \n",
    "\n",
    "  df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    # 'cluster' : cluster_assignment,\n",
    "    'source': source,\n",
    "    'page': page,\n",
    "    'scores': scores,\n",
    "    'text': text,\n",
    "    'idDANE': idDN\n",
    "    })\n",
    "  \n",
    "  df = df.merge(codigos_dane_dpto[['CodigoDepartamento','NombreDepartamento']], 'left', left_on='idDANE', right_on='CodigoDepartamento')\n",
    "  \n",
    "  imgs = []\n",
    "  for index, row in df.iterrows():\n",
    "    # imgs.append(readZip(row['idDANE'], row['page']))\n",
    "    readZip(row['idDANE'], row['page'])\n",
    "  png_files = [f for f in os.listdir((os.path.join(os.getcwd(), os.pardir, \"data\", \"temp\", \"img\"))) if f.startswith('page_')]\n",
    "  # png_files = [f for f in os.listdir('/content/images') if f.startswith('page_')]\n",
    "  for file in png_files:\n",
    "    # image_path = os.path.join('/content/images', file)\n",
    "    image_path = os.path.join((os.path.join(os.getcwd(), os.pardir, \"data\", \"temp\", \"img\", file)))\n",
    "    image = Image.open(image_path)\n",
    "    image = [image, file]\n",
    "    imgs.append(image)\n",
    "    # print(row['idDANE'], row['NombreDepartamento'])\n",
    "\n",
    "  ## Clusters\n",
    "  labels = [f\"{a}-{b}\" for a, b in zip(page, df['NombreDepartamento'])]\n",
    "  # labels = [f\"{a}-{b}: {c}\" for a, b, c in zip(df['NombreDepartamento'],page,scores)]\n",
    "  text.append(query)\n",
    "  labels.append('query')\n",
    "\n",
    "  results_embeddings = model.encode(text,batch_size=64,show_progress_bar=True, device='cpu')  \n",
    "\n",
    "  emb_results = np.array(results_embeddings)\n",
    "  print(f'3: {len(emb_results)}')\n",
    "\n",
    "  tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "  embeddings_2d = tsne.fit_transform(emb_results)\n",
    "  print(f'4: {len(embeddings_2d)}')\n",
    "\n",
    "  num_clusters = clusters\n",
    "  clustering_model = KMeans(n_clusters=num_clusters)\n",
    "  clustering_model.fit(emb_results)\n",
    "  cluster_assignment = clustering_model.labels_\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "  plt.figure(figsize=(6, 4))\n",
    "  colors = ['r', 'g', 'b', 'y', 'c', 'm']\n",
    "\n",
    "  for index, embedding in enumerate(embeddings_2d):\n",
    "      # print(f'{index} - {embedding}')\n",
    "      plt.scatter(embedding[0],embedding[1],color=colors[cluster_assignment[index]])\n",
    "\n",
    "  plt.xlabel(\"X\")\n",
    "  plt.ylabel(\"Y\")\n",
    "  plt.title(\"Grupos\")\n",
    "\n",
    "  \n",
    "\n",
    "  labels_plt = labels.copy()\n",
    "  labels_plt.append('query')\n",
    "  print(len(labels_plt))\n",
    "\n",
    "  for i, sentence in enumerate(labels):\n",
    "      plt.annotate(sentence, (embeddings_2d[i,0],embeddings_2d[i,1]))\n",
    "\n",
    "  \n",
    "  plt.grid(False)\n",
    "  ruta_img = os.path.join(current_dir, os.pardir, \"data\",\"temp\",\"img\")\n",
    "  # ruta_img = '/content/images'\n",
    "  plt.savefig(os.path.join(ruta_img, \"grupos.png\"))\n",
    "\n",
    "  \n",
    "\n",
    "  img_cl = []  \n",
    "  png_files = [f for f in os.listdir(ruta_img) if f.endswith('grupos.png')]\n",
    "  for file in png_files:\n",
    "    # image_path = os.path.join(ruta_img, file)\n",
    "    # image_path = os.path.join('/content/images', file)\n",
    "    image_path = os.path.join(ruta_img, file)\n",
    "    image = Image.open(image_path)\n",
    "    image = [image, file]\n",
    "    img_cl.append(image)\n",
    "\n",
    "  df[['source','page']] = df[['source','page']].astype(str)\n",
    "  df['LABELS'] = df['source'] + '-' + df['page']\n",
    "  df['cluster'] = cluster_assignment[:-1]\n",
    "  \n",
    "  \n",
    "\n",
    "  return df[['cluster','idDANE','page','scores','NombreDepartamento','text']], img_cl, imgs\n",
    "\n",
    "# pr = search('desarrollo equilibrado asentamientos, pueblos, ciudades','no filtro','x',10)\n",
    "# Define possible genres\n",
    "# clientes = clients.tolist()\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=search,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=5, placeholder=\"Escribe aquí tu consulta...\", label=\"Consulta\"),\n",
    "        gr.Radio([\"Filtro\", \"Sin filtro\"]),\n",
    "        # gr.Textbox(lines=1, placeholder=\"Doc Name\", label=\"Doc Name\"),\n",
    "        gr.Dropdown(choices=list(codigos_dane_dpto.NombreDepartamento), label=\"Departamento\"),\n",
    "        # gr.Slider(minimum=1, maximum=10, value=5, label=\"Puntuación mínima\"),\n",
    "        gr.Number(minimum=1, maximum=15, value=10, label=\"Número de resultados\")\n",
    "\n",
    "    ],\n",
    "    outputs=[gr.Dataframe(type=\"pandas\", label=\"Resultados\"), gr.Gallery(\n",
    "        label=\"Generated images\", show_label=True, elem_id=\"gallery\"\n",
    "    , columns=[2], rows=[5], object_fit=\"contain\", height=\"auto\")],\n",
    "    title=\"PDT Gobernaciones\",\n",
    "    description=\"Analizar PDT.\",\n",
    "    live=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'page': 19, 'source': '17.pdf'}, page_content='desarrollo vigencia'),\n",
       "  25.232271194458008),\n",
       " (Document(metadata={'page': 317, 'source': '23.pdf'}, page_content='actualmente llamado cumplir'),\n",
       "  32.78734588623047),\n",
       " (Document(metadata={'page': 98, 'source': '17.pdf'}, page_content='xx'),\n",
       "  33.443660736083984),\n",
       " (Document(metadata={'page': 166, 'source': '94.pdf'}, page_content='incremento ano respecto vigencia'),\n",
       "  36.31009292602539),\n",
       " (Document(metadata={'page': 23, 'source': '15.pdf'}, page_content='territorial ultima vigencia'),\n",
       "  38.14746856689453),\n",
       " (Document(metadata={'page': 256, 'source': '17.pdf'}, page_content='xxx'),\n",
       "  38.73954391479492),\n",
       " (Document(metadata={'page': 36, 'source': '76.pdf'}, page_content='futuras puedan gozar'),\n",
       "  39.09825897216797),\n",
       " (Document(metadata={'page': 287, 'source': '17.pdf'}, page_content='telefonia movil intervenir bienes interes cultural departamento primeros auxilios identificar valorar conservar difundir'),\n",
       "  41.879112243652344),\n",
       " (Document(metadata={'page': 174, 'source': '88.pdf'}, page_content='mano obra nuevo comienzo'),\n",
       "  42.640716552734375),\n",
       " (Document(metadata={'page': 69, 'source': '81.pdf'}, page_content='realicen giro recursos den respuesta numeral articulo ley fin contribuir seguridad creador gestor cultural anterior espera'),\n",
       "  42.85871124267578)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search('desarrollo equilibrado','no','dpto',20,dir_temp_img=dir_temp_img)\n",
    "query = [\"desarrollo \",\"garantizan un desarrollo territorial equilibrado\"]\n",
    "\n",
    "\n",
    "docs = vectorstore_chroma.similarity_search_with_score(query[0], k=10)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(lines=5, placeholder=\"Escribe aquí tu consulta...\", label=\"Consulta\")\n",
    "            gr.Examples([\"dinámica población\",\"desarrollo territorial equilibrado\", \"margen fiscal local\"], inputs=[prompt])\n",
    "        with gr.Column():\n",
    "            filtros = gr.Radio([\"Filtro\", \"Sin filtro\"], label=\"Filtros\")\n",
    "            departamento = gr.Dropdown(choices=list(codigos_dane_dpto.NombreDepartamento), label=\"Departamento\")\n",
    "            salidas = gr.Number(minimum=1, maximum=15, value=10, label=\"Número de resultados\")\n",
    "            clusters = gr.Number(minimum=1, maximum=5, value=3, label=\"Número de Clusters\")\n",
    "            \n",
    "\n",
    "    with gr.Row():\n",
    "        consulta_btn = translate_btn = gr.Button(value=\"Consultar\")\n",
    "\n",
    "    with gr.Row():\n",
    "        tabla = gr.Dataframe(type=\"pandas\", label=\"Resultados\")\n",
    "\n",
    "    # with gr.Row():\n",
    "    #     resol = gr.Slider(minimum=500, maximum=1500, value=100, label=\"Resolución\")\n",
    "\n",
    "    with gr.Row():\n",
    "        galeria = gr.Gallery(label=\"Clusters\", show_label=True, elem_id=\"gallery\", scale=1,object_fit=\"contain\", height=500)#\"auto\")\n",
    "\n",
    "    with gr.Row():\n",
    "        galeria1 = gr.Gallery(label=\"Hoja PDT\", show_label=True, elem_id=\"gallery1\", scale=2,object_fit=\"contain\", height=1000)#\"auto\")\n",
    "\n",
    "\n",
    "    translate_btn.click(search, inputs=[prompt, filtros, departamento, salidas, clusters], outputs=[tabla, galeria, galeria1])\n",
    "    Button.click(demo.clos)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: 11\n",
      "4: 11\n",
      "12\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\dfsandovalp\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\nlp_base\\\\python39.zip',\n",
       " 'c:\\\\Users\\\\dfsandovalp\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\nlp_base\\\\DLLs',\n",
       " 'c:\\\\Users\\\\dfsandovalp\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\nlp_base\\\\lib',\n",
       " 'c:\\\\Users\\\\dfsandovalp\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\nlp_base',\n",
       " '',\n",
       " 'C:\\\\Users\\\\dfsandovalp\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages',\n",
       " 'c:\\\\Users\\\\dfsandovalp\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\nlp_base\\\\lib\\\\site-packages',\n",
       " 'c:\\\\Users\\\\dfsandovalp\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\nlp_base\\\\lib\\\\site-packages\\\\win32',\n",
       " 'c:\\\\Users\\\\dfsandovalp\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\nlp_base\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'c:\\\\Users\\\\dfsandovalp\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\nlp_base\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'c:\\\\Users\\\\dfsandovalp\\\\Documents\\\\AUTOMATIZACIONES\\\\ODS\\\\Disponer\\\\vdb_ods_pdt_co\\\\Notebooks\\\\..']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from src.utils import *\n",
    "# import src\n",
    "# print(os.path.exists(os.path.join(sys.path[-1],\"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = sys.path[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dfsandovalp\\AppData\\Local\\miniconda3\\envs\\nlp_base\\lib\\site-packages\\setuptools\\_vendor\\src\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "listdir: path should be string, bytes, os.PathLike or None, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sys\u001b[38;5;241m.\u001b[39mpath[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sys\u001b[38;5;241m.\u001b[39mpath[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: listdir: path should be string, bytes, os.PathLike or None, not list"
     ]
    }
   ],
   "source": [
    "# print(os.path.join(sys.path[-1],'src'))\n",
    "# print(os.listdir(sys.path))\n",
    "# os.path.exists(os.path.join(sys.path[-1],'src'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
